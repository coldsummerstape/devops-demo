app:
  enabled: true
  controllers:
    app:
      initContainers:
        wait-for-redis:
          image:
            repository: busybox
            tag: "1.36"
            pullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - >-
              until nc -z "$REDIS_HOST" "$REDIS_PORT"; do echo "waiting for redis at ${REDIS_HOST}:${REDIS_PORT}"; sleep 2; done
          env:
            - name: REDIS_HOST
              value: "{{ .Release.Name }}-redis-master"
            - name: REDIS_PORT
              value: "6379"
      pod:
        annotations:
          promtail.grafana.com/scrape: "true"
          promtail.grafana.com/job: "devops-demo"
          promtail.grafana.com/pipeline_stages: |
            - cri: {}
      containers:
        app:
          image:
            repository: ghcr.io/coldsummerstape/devops-demo
            tag: latest
            pullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 3000
          env:
            - name: NODE_ENV
              value: production
            - name: REDIS_HOST
              value: "{{ .Release.Name }}-redis-master"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DB
              value: "0"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: "{{ .Release.Name }}-redis"
                  key: redis-password
          probes:
            liveness:
              enabled: true
              custom: true
              spec:
                httpGet: { path: /, port: 3000 }
            readiness:
              enabled: true
              custom: true
              spec:
                httpGet: { path: /, port: 3000 }
  service:
    main:
      enabled: true
      controller: app
      ports:
        http:
          port: 3000
          targetPort: http
  ingress:
    main:
      enabled: true
      className: nginx
      hosts:
        - host: devops-demo.local
          paths:
            - path: /
              pathType: Prefix
              service:
                name: devops-demo
                port: 3000
      tls: []

  resources:
    limits: { cpu: 500m, memory: 512Mi }
    requests: { cpu: 100m, memory: 128Mi }
  autoscaling:
    horizontal:
      app:
        enabled: true
        minReplicas: 2
        maxReplicas: 5
        targetCPUUtilizationPercentage: 70
  podSecurityContext:
    runAsNonRoot: true
  securityContext:
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities: { drop: ["ALL"] }

redis:
  enabled: true
  architecture: standalone
  auth:
    enabled: true
  master:
    persistence:
      enabled: true
      storageClass: "fast.kz-1a"
      size: 8Gi
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      labels: { release: prometheus }

# --- KUBE-PROMETHEUS-STACK (Prometheus + Grafana) ---

monitoringstack:
  enabled: true

  prometheus:
    prometheusSpec:
      retention: 15d

      # ВАЖНО: подбираем ВСЕ ServiceMonitor/PodMonitor из ВСЕХ ns
      # (без фильтра по метке, чтобы видеть встроенные SM из стека)
      serviceMonitorSelector: {}
      podMonitorSelector: {}
      ruleSelector: {}

      serviceMonitorNamespaceSelector: {}
      podMonitorNamespaceSelector: {}
      ruleNamespaceSelector: {}

      # (явно зафиксируем "не использовать helm labels", чтобы пустой selector означал ВСЕ)
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false

  kubeStateMetrics:
    enabled: true
  nodeExporter:
    enabled: true

  grafana:
    enabled: true

    # Включаем sidecar для автоматического создания datasources
    sidecar:
      datasources:
        enabled: true
        label: grafana_datasource
        labelValue: "1"
        searchNamespace: ALL
        resource: both
        # Отключаем автосоздание дефолтного Prometheus сайдкаром; источники задаём вручную ниже
        defaultDatasourceEnabled: false
        isDefaultDatasource: false

    # Используем sidecar способ настройки datasources
    additionalDataSources:
      - name: Prometheus
        type: prometheus
        uid: prometheus
        url: "http://devops-demo-monitoringstac-prometheus:9090/"
        access: proxy
        isDefault: true
        jsonData:
          httpMethod: POST
          timeInterval: 30s
      - name: Alertmanager
        type: alertmanager
        uid: alertmanager
        url: "http://devops-demo-monitoringstac-alertmanager.devops-test:9093/"
        access: proxy
        jsonData:
          handleGrafanaManagedAlerts: false
          implementation: prometheus
      - name: Loki
        type: loki
        uid: loki
        url: "http://devops-demo-loki:3100"
        access: proxy
        isDefault: false
        jsonData:
          maxLines: 1000

    env:
      GF_USERS_AUTO_ASSIGN_ORG: "true"
      GF_USERS_AUTO_ASSIGN_ORG_ID: "1"
      GF_USERS_AUTO_ASSIGN_ROLE: "Admin"

    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: nginx
      hosts: [grafana.devops-demo.local]
      path: /
      pathType: Prefix
      tls: []

    persistence:
      enabled: true
      storageClassName: "fast.kz-1a"
      size: 1Gi

# --- LOKI-STACK ---

loggingstack:
  # Отключаем создание Loki datasource в саб-чарте loki-stack, чтобы избежать конфликта
  grafana:
    sidecar:
      datasources:
        enabled: false
  
  promtail:
    # (опционально) включён по умолчанию в loki-stack; оставьте/уберите по ситуации
    enabled: true

    # Полная конфигурация promtail
    config:
      # (не обязательно, но удобно иметь явный файл позиций)
      positions:
        filename: /run/promtail/positions.yaml

      # Куда отправлять логи — ИСПРАВЛЕНО
      clients:
        - url: http://devops-demo-loki:3100/loki/api/v1/push

      # (опционально) http-сервер promtail для /metrics и /ready
      server:
        http_listen_port: 3101
        grpc_listen_port: 0

      scrape_configs:
        - job_name: kubernetes-pods-annotated

          # pipeline: парсинг CRI-формата, затем ваш шаблон NestJS,
          # добавление меток, установка таймстемпа и замена output
          pipeline_stages:
            - cri: {}
            - regex:
                expression: '^\[Nest\]\s+\d+\s+-\s+(?P<ts>\d{2}/\d{2}/\d{4},\s+\d{1,2}:\d{2}:\d{2}\s+[AP]M)\s+(?P<level>[A-Z]+)\s+\[(?P<context>[^\]]+)\]\s+(?P<msg>.*)$'
            - labels:
                level:
                context:
            - timestamp:
                source: ts
                format: '01/02/2006, 3:04:05 PM'
                # если логи в UTC — оставляем; если нет, можно указать location
                # location: 'UTC'
            - output:
                source: msg

          kubernetes_sd_configs:
            - role: pod

          relabel_configs:
            # брать только поды с аннотацией promtail.grafana.com/scrape=true
            - action: keep
              source_labels: [__meta_kubernetes_pod_annotation_promtail_grafana_com_scrape]
              regex: true

            # установить label job из аннотации promtail.grafana.com/job
            - action: replace
              source_labels: [__meta_kubernetes_pod_annotation_promtail_grafana_com_job]
              target_label: job

            # полезные стандартные лейблы
            - action: replace
              source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
            - action: replace
              source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
            - action: replace
              source_labels: [__meta_kubernetes_pod_container_name]
              target_label: container

            # путь к логам контейнеров (CRI)
            - action: replace
              replacement: /var/log/pods/*/*/*.log
              target_label: __path__

# --- INGRESS-NGINX (ИСПРАВЛЕННЫЙ alias!) ---

ingressnginx:            # <— было ingressNginx, из-за этого метрики не включались
  enabled: true
  controller:
    ingressClassResource:
      name: nginx
      controllerValue: k8s.io/ingress-nginx
      default: true
    service:
      type: LoadBalancer

    metrics:
      enabled: true
      serviceMonitor:
        enabled: true     # чарт сам создаст ServiceMonitor
        additionalLabels: {}  # метки не нужны, Prometheus берёт все SM